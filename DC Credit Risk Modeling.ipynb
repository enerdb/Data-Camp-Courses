{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Preparing Loan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding credit risk\n",
    "\n",
    "Expected loss = PD * EAD * LGD\n",
    "* PD - Probability of Default\n",
    "* EAD - Exposure at Default\n",
    "* LGD - Loss Given Default (ratio lost when we sell a debt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the data\n",
    "print(cr_loan.dtypes)\n",
    "\n",
    "# Check the first five rows of the data\n",
    "print(cr_loan.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of loan amounts with a histogram\n",
    "n, bins, patches = plt.hist(x=cr_loan['loan_amnt'], bins='auto', color='blue',alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel(\"Loan Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are 32 000 rows of data so the scatter plot may take a little while to plot.\")\n",
    "\n",
    "# Plot a scatter plot of income against age\n",
    "plt.scatter(cr_loan['person_income'], cr_loan['person_age'],c='blue', alpha=0.5)\n",
    "plt.xlabel('Personal Income')\n",
    "plt.ylabel('Persone Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstab and pivot tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross table of the loan intent and loan status\n",
    "print(pd.crosstab(cr_loan['loan_intent'], cr_loan['loan_status'], margins = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross table of home ownership, loan status, and grade\n",
    "print(pd.crosstab(cr_loan['person_home_ownership'],[cr_loan['loan_status'],cr_loan['loan_grade']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross table of home ownership, loan status, and average percent income\n",
    "print(pd.crosstab(cr_loan['person_home_ownership'], cr_loan['loan_status'],\n",
    "              values=cr_loan['loan_percent_income'], aggfunc='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of percentage income by loan status\n",
    "cr_loan.boxplot(column = ['loan_percent_income'], by = 'loan_status')\n",
    "plt.title('Average Percent Income by Loan Status')\n",
    "plt.suptitle('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers in credit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding outliers with cross tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cross table for loan status, home ownership, and the max employment length\n",
    "print(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n",
    "                  values=cr_loan['person_emp_length'], aggfunc='max'))\n",
    "\n",
    "# Create an array of indices where employment length is greater than 60\n",
    "indices = cr_loan[cr_loan['person_emp_length'] > 60].index\n",
    "\n",
    "# Drop the records from the data based on the indices and create a new dataframe\n",
    "cr_loan_new = cr_loan.drop(indices)\n",
    "\n",
    "# Create the cross table from earlier and include minimum employment length\n",
    "print(pd.crosstab(cr_loan_new['loan_status'],cr_loan_new['person_home_ownership'],\n",
    "            values=cr_loan_new['person_emp_length'], aggfunc=['min','max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing credit outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot for age and amount\n",
    "plt.scatter(cr_loan['person_age'], cr_loan['loan_amnt'], c='blue', alpha=0.5)\n",
    "plt.xlabel(\"Person Age\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to drop the record from the data frame and create a new one\n",
    "cr_loan_new = cr_loan.drop(cr_loan[cr_loan['person_age'] > 90].index)\n",
    "\n",
    "# Create a scatter plot of age and interest rate\n",
    "colors = [\"blue\",\"red\"]\n",
    "plt.scatter(cr_loan_new['person_age'], cr_loan_new['loan_int_rate'],\n",
    "            c = cr_loan_new['loan_status'],\n",
    "            cmap = matplotlib.colors.ListedColormap(colors),\n",
    "            alpha=0.5)\n",
    "plt.xlabel(\"Person Age\")\n",
    "plt.ylabel(\"Loan Interest Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk with missing data in loan data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing missing credit data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a null value column array\n",
    "print(cr_loan.columns[cr_loan.isnull().any()])\n",
    "\n",
    "# Print the top five rows with nulls for employment length\n",
    "print(cr_loan[cr_loan['person_emp_length'].isnull()].head())\n",
    "\n",
    "# Impute the null values with the median value for all employment lengths\n",
    "cr_loan['person_emp_length'].fillna((cr_loan['person_emp_length'].median()), inplace=True)\n",
    "\n",
    "# Create a histogram of employment length\n",
    "n, bins, patches = plt.hist(cr_loan['person_emp_length'], bins='auto', color='blue')\n",
    "plt.xlabel(\"Person Employment Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of nulls\n",
    "print(cr_loan['loan_int_rate'].isna().sum())\n",
    "\n",
    "# Store the array on indices\n",
    "indices = cr_loan[cr_loan['loan_int_rate'].isna()].index\n",
    "\n",
    "# Save the new data without missing data\n",
    "cr_loan_clean = cr_loan.drop(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression for probability of default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y data sets\n",
    "X = cr_loan_clean[['loan_int_rate','person_emp_length','person_income']]\n",
    "y = cr_loan_clean[['loan_status']]\n",
    "\n",
    "# Use test_train_split to create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "clf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Print the models coefficients\n",
    "print(clf_logistic.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the probability of default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of each training set\n",
    "print(X1_train.head())\n",
    "print(X2_train.head())\n",
    "\n",
    "# Create and train a model on the first training data\n",
    "clf_logistic1 = LogisticRegression(solver='lbfgs').fit(X1_train, np.ravel(y_train))\n",
    "\n",
    "# Create and train a model on the second training data\n",
    "clf_logistic2 = LogisticRegression(solver='lbfgs').fit(X2_train, np.ravel(y_train))\n",
    "\n",
    "# Print the coefficients of each model\n",
    "print(clf_logistic1.coef_)\n",
    "print(clf_logistic2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two data sets for numeric and non-numeric data\n",
    "cred_num = cr_loan_clean.select_dtypes(exclude=['object'])\n",
    "cred_str = cr_loan_clean.select_dtypes(include=['object'])\n",
    "\n",
    "# One-hot encode the non-numeric columns\n",
    "cred_str_onehot = pd.get_dummies(cred_str)\n",
    "\n",
    "# Union the one-hot encoded columns to the numeric ones\n",
    "cr_loan_prep = pd.concat([cred_num, cred_str_onehot], axis=1)\n",
    "\n",
    "# Print the columns in the new data set\n",
    "print(cr_loan_prep.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting probability of default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model on the training data\n",
    "clf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Create predictions of probability for loan status using test data\n",
    "preds = clf_logistic.predict_proba(X_test)\n",
    "\n",
    "# Create dataframes of first five predictions, and first five true labels\n",
    "preds_df = pd.DataFrame(preds[:,1][0:5], columns = ['prob_default'])\n",
    "true_df = y_test.head()\n",
    "\n",
    "# Concatenate and print the two data frames for comparison\n",
    "print(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit model performance\n",
    "\n",
    "Accuracy (.score(X_test, y_test)) = Number of correct predictions / Number of predictions\n",
    "\n",
    "Sensitivity = True positive rate\n",
    "\n",
    "Fall-out = false positive rate\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1-score\n",
    "\n",
    "Support\n",
    "\n",
    "Threshold = limit probability where we predict 0 or 1.\n",
    "\n",
    "roc_auc_score : Area under curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default classification reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the probabilities of default\n",
    "preds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n",
    "\n",
    "# Reassign loan status based on the threshold\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Print the row counts for each loan status\n",
    "print(preds_df['loan_status'].value_counts())\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df['loan_status'], target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Report metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df['loan_status'], target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the non-average values from the report\n",
    "print(precision_recall_fscore_support(y_test,preds_df['loan_status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually scoring credit models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions and store them in a variable\n",
    "preds = clf_logistic.predict_proba(X_test)\n",
    "\n",
    "# Print the accuracy score the model\n",
    "print(clf_logistic.score(X_test, y_test))\n",
    "\n",
    "# Plot the ROC curve of the probabilities of default\n",
    "prob_default = preds[:, 1]\n",
    "fallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\n",
    "plt.plot(fallout, sensitivity, color = 'darkorange')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Compute the AUC and store it in a variable\n",
    "auc = roc_auc_score(y_test, prob_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model discrimination and impact\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "| Predicted = 0| Predicted = 1 |\n",
    "| --- | --- |\n",
    "| True Negatives (Predicted = 0, Actual = 0)| False Positives (Predicted = 1, Actual - 0)|\n",
    "| False Negatives (Predicted = 0, Actual = 1)| True Positives (Predicted = 1, Actual = 1)|\n",
    "\n",
    "Precision_0 = TN / (TN + FN)\n",
    "\n",
    "Precision_1 = TP / (TP + FP)\n",
    "\n",
    "Recall_0 = TN / (TN + FP)\n",
    "\n",
    "Recall_1 = TP / (TP + FN) = Proportion of true defaults predicted = Sensitivity\n",
    "\n",
    "F1_score = 2 * (Precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholds and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for defaults to 0.5\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test,preds_df['loan_status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How thresholds affect performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the values of loan status based on the new threshold\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.4 else 0)\n",
    "\n",
    "# Store the number of loan defaults from the prediction data\n",
    "num_defaults = preds_df['loan_status'].value_counts()[1]\n",
    "\n",
    "# Store the default recall from the classification report\n",
    "default_recall = precision_recall_fscore_support(y_test,preds_df['loan_status'])[1][1]\n",
    "\n",
    "# Calculate the estimated impact of the new default recall rate\n",
    "print(avg_loan_amnt * num_defaults * (1 - default_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresh,def_recalls)\n",
    "plt.plot(thresh,nondef_recalls)\n",
    "plt.plot(thresh,accs)\n",
    "plt.xlabel(\"Probability Threshold\")\n",
    "plt.xticks(ticks)\n",
    "plt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees Using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted trees with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees for defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "import xgboost as xgb\n",
    "clf_gbt = xgb.XGBClassifier().fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Predict with a model\n",
    "gbt_preds = clf_gbt.predict_proba(X_test)\n",
    "\n",
    "# Create dataframes of first five predictions, and first five true labels\n",
    "preds_df = pd.DataFrame(gbt_preds[:,1][0:5], columns = ['prob_default'])\n",
    "true_df = y_test.head()\n",
    "\n",
    "# Concatenate and print the two data frames for comparison\n",
    "print(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted portfolio performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of the portfolio data frame\n",
    "print(portfolio.head())\n",
    "\n",
    "# Create expected loss columns for each model using the formula\n",
    "portfolio['lr_expected_loss'] = portfolio['lr_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n",
    "portfolio['gbt_expected_loss'] = portfolio['gbt_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n",
    "\n",
    "# Print the sum of the expected loss for lr\n",
    "print('LR expected loss: ', np.sum(portfolio['lr_expected_loss']))\n",
    "\n",
    "# Print the sum of the expected loss for gbt\n",
    "print('GBT expected loss: ', np.sum(portfolio['gbt_expected_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing gradient boosted trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for loan status\n",
    "gbt_preds = clf_gbt.predict(X_test)\n",
    "\n",
    "# Check the values created by the predict method\n",
    "print(gbt_preds)\n",
    "\n",
    "# Print the classification report of the model\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column selection for credit risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column importance and default prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model on the training data\n",
    "clf_gbt = xgb.XGBClassifier().fit(X_train,np.ravel(y_train))\n",
    "\n",
    "# Print the column importances from the model\n",
    "print(clf_gbt.get_booster().get_score(importance_type = 'weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing column importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the X data with 2 columns\n",
    "clf_gbt2 = xgb.XGBClassifier().fit(X2_train,np.ravel(y_train))\n",
    "\n",
    "# Plot the column importance for this model\n",
    "xgb.plot_importance(clf_gbt2, importance_type = 'weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column selection and model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the loan_status using each model\n",
    "gbt_preds = gbt.predict(X_test)\n",
    "gbt2_preds = gbt2.predict(X2_test)\n",
    "\n",
    "# Print the classification report of the first model\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))\n",
    "\n",
    "# Print the classification report of the second model\n",
    "print(classification_report(y_test, gbt2_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation for credit models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating credit models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eval_metric': 'auc', 'objective': 'binary:logistic', 'seed': 123}\n",
    "\n",
    "# Set the values for number of folds and stopping iterations\n",
    "n_folds = 5\n",
    "early_stopping = 10\n",
    "\n",
    "# Create the DTrain matrix for XGBoost\n",
    "DTrain = xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "# Create the data frame of cross validations\n",
    "cv_df = xgb.cv(params, DTrain, num_boost_round = 5, nfold=n_folds,\n",
    "            early_stopping_rounds=early_stopping)\n",
    "\n",
    "# Print the cross validations data frame\n",
    "print(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits to cross-validation testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of the CV results data frame\n",
    "print(cv_results_big.head())\n",
    "\n",
    "# Calculate the mean of the test AUC scores\n",
    "print(np.mean(cv_results_big['test-auc-mean']).round(2))\n",
    "\n",
    "# Plot the test AUC scores for each iteration\n",
    "plt.plot(cv_results_big['test-auc-mean'])\n",
    "plt.title('Test AUC Score Over 600 Iterations')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Test AUC Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test AUC score never quite reaches 1.0 and begins to decrease slightly after 100 iterations. This is because this much cross-validation can actually cause the model to become overfit. So, there is a limit to how much cross-validation you should to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradient boosted tree model using two hyperparameters\n",
    "gbt = xgb.XGBClassifier(learning_rate = 0.1, max_depth = 7)\n",
    "\n",
    "# Calculate the cross validation scores for 4 folds\n",
    "cv_scores = cross_val_score(gbt, X_train, np.ravel(y_train), cv = 4)\n",
    "\n",
    "# Print the cross validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "# Print the average accuracy and standard deviation of the scores\n",
    "print(\"Average accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(),\n",
    "                                              cv_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance in loan data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sets for defaults and non-defaults\n",
    "nondefaults = X_y_train[X_y_train['loan_status'] == 0]\n",
    "defaults = X_y_train[X_y_train['loan_status'] == 1]\n",
    "\n",
    "# Undersample the non-defaults\n",
    "nondefaults_under = nondefaults.sample(count_default)\n",
    "\n",
    "# Concatenate the undersampled nondefaults with defaults\n",
    "X_y_train_under = pd.concat([nondefaults_under.reset_index(drop = True),\n",
    "                             defaults.reset_index(drop = True)], axis = 0)\n",
    "\n",
    "# Print the value counts for loan status\n",
    "print(X_y_train_under['loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled tree performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the classification reports\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))\n",
    "print(classification_report(y_test, gbt2_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix for both old and new models\n",
    "print(confusion_matrix(y_test,gbt_preds))\n",
    "print(confusion_matrix(y_test,gbt2_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and compare the AUC scores of the old and new models\n",
    "print(roc_auc_score(y_test, gbt_preds))\n",
    "print(roc_auc_score(y_test, gbt2_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
