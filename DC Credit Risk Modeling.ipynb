{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Preparing Loan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding credit risk\n",
    "\n",
    "Expected loss = PD * EAD * LGD\n",
    "* PD - Probability of Default\n",
    "* EAD - Exposure at Default\n",
    "* LGD - Loss Given Default (ratio lost when we sell a debt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the data\n",
    "print(cr_loan.dtypes)\n",
    "\n",
    "# Check the first five rows of the data\n",
    "print(cr_loan.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of loan amounts with a histogram\n",
    "n, bins, patches = plt.hist(x=cr_loan['loan_amnt'], bins='auto', color='blue',alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel(\"Loan Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are 32 000 rows of data so the scatter plot may take a little while to plot.\")\n",
    "\n",
    "# Plot a scatter plot of income against age\n",
    "plt.scatter(cr_loan['person_income'], cr_loan['person_age'],c='blue', alpha=0.5)\n",
    "plt.xlabel('Personal Income')\n",
    "plt.ylabel('Persone Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstab and pivot tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross table of the loan intent and loan status\n",
    "print(pd.crosstab(cr_loan['loan_intent'], cr_loan['loan_status'], margins = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross table of home ownership, loan status, and grade\n",
    "print(pd.crosstab(cr_loan['person_home_ownership'],[cr_loan['loan_status'],cr_loan['loan_grade']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross table of home ownership, loan status, and average percent income\n",
    "print(pd.crosstab(cr_loan['person_home_ownership'], cr_loan['loan_status'],\n",
    "              values=cr_loan['loan_percent_income'], aggfunc='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of percentage income by loan status\n",
    "cr_loan.boxplot(column = ['loan_percent_income'], by = 'loan_status')\n",
    "plt.title('Average Percent Income by Loan Status')\n",
    "plt.suptitle('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers in credit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding outliers with cross tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cross table for loan status, home ownership, and the max employment length\n",
    "print(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n",
    "                  values=cr_loan['person_emp_length'], aggfunc='max'))\n",
    "\n",
    "# Create an array of indices where employment length is greater than 60\n",
    "indices = cr_loan[cr_loan['person_emp_length'] > 60].index\n",
    "\n",
    "# Drop the records from the data based on the indices and create a new dataframe\n",
    "cr_loan_new = cr_loan.drop(indices)\n",
    "\n",
    "# Create the cross table from earlier and include minimum employment length\n",
    "print(pd.crosstab(cr_loan_new['loan_status'],cr_loan_new['person_home_ownership'],\n",
    "            values=cr_loan_new['person_emp_length'], aggfunc=['min','max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing credit outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot for age and amount\n",
    "plt.scatter(cr_loan['person_age'], cr_loan['loan_amnt'], c='blue', alpha=0.5)\n",
    "plt.xlabel(\"Person Age\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to drop the record from the data frame and create a new one\n",
    "cr_loan_new = cr_loan.drop(cr_loan[cr_loan['person_age'] > 90].index)\n",
    "\n",
    "# Create a scatter plot of age and interest rate\n",
    "colors = [\"blue\",\"red\"]\n",
    "plt.scatter(cr_loan_new['person_age'], cr_loan_new['loan_int_rate'],\n",
    "            c = cr_loan_new['loan_status'],\n",
    "            cmap = matplotlib.colors.ListedColormap(colors),\n",
    "            alpha=0.5)\n",
    "plt.xlabel(\"Person Age\")\n",
    "plt.ylabel(\"Loan Interest Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk with missing data in loan data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing missing credit data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a null value column array\n",
    "print(cr_loan.columns[cr_loan.isnull().any()])\n",
    "\n",
    "# Print the top five rows with nulls for employment length\n",
    "print(cr_loan[cr_loan['person_emp_length'].isnull()].head())\n",
    "\n",
    "# Impute the null values with the median value for all employment lengths\n",
    "cr_loan['person_emp_length'].fillna((cr_loan['person_emp_length'].median()), inplace=True)\n",
    "\n",
    "# Create a histogram of employment length\n",
    "n, bins, patches = plt.hist(cr_loan['person_emp_length'], bins='auto', color='blue')\n",
    "plt.xlabel(\"Person Employment Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of nulls\n",
    "print(cr_loan['loan_int_rate'].isna().sum())\n",
    "\n",
    "# Store the array on indices\n",
    "indices = cr_loan[cr_loan['loan_int_rate'].isna()].index\n",
    "\n",
    "# Save the new data without missing data\n",
    "cr_loan_clean = cr_loan.drop(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression for probability of default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y data sets\n",
    "X = cr_loan_clean[['loan_int_rate','person_emp_length','person_income']]\n",
    "y = cr_loan_clean[['loan_status']]\n",
    "\n",
    "# Use test_train_split to create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "clf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Print the models coefficients\n",
    "print(clf_logistic.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the probability of default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of each training set\n",
    "print(X1_train.head())\n",
    "print(X2_train.head())\n",
    "\n",
    "# Create and train a model on the first training data\n",
    "clf_logistic1 = LogisticRegression(solver='lbfgs').fit(X1_train, np.ravel(y_train))\n",
    "\n",
    "# Create and train a model on the second training data\n",
    "clf_logistic2 = LogisticRegression(solver='lbfgs').fit(X2_train, np.ravel(y_train))\n",
    "\n",
    "# Print the coefficients of each model\n",
    "print(clf_logistic1.coef_)\n",
    "print(clf_logistic2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two data sets for numeric and non-numeric data\n",
    "cred_num = cr_loan_clean.select_dtypes(exclude=['object'])\n",
    "cred_str = cr_loan_clean.select_dtypes(include=['object'])\n",
    "\n",
    "# One-hot encode the non-numeric columns\n",
    "cred_str_onehot = pd.get_dummies(cred_str)\n",
    "\n",
    "# Union the one-hot encoded columns to the numeric ones\n",
    "cr_loan_prep = pd.concat([cred_num, cred_str_onehot], axis=1)\n",
    "\n",
    "# Print the columns in the new data set\n",
    "print(cr_loan_prep.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting probability of default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model on the training data\n",
    "clf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Create predictions of probability for loan status using test data\n",
    "preds = clf_logistic.predict_proba(X_test)\n",
    "\n",
    "# Create dataframes of first five predictions, and first five true labels\n",
    "preds_df = pd.DataFrame(preds[:,1][0:5], columns = ['prob_default'])\n",
    "true_df = y_test.head()\n",
    "\n",
    "# Concatenate and print the two data frames for comparison\n",
    "print(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit model performance\n",
    "\n",
    "Accuracy (.score(X_test, y_test)) = Number of correct predictions / Number of predictions\n",
    "\n",
    "Sensitivity = True positive rate\n",
    "\n",
    "Fall-out = false positive rate\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1-score\n",
    "\n",
    "Support\n",
    "\n",
    "Threshold = limit probability where we predict 0 or 1.\n",
    "\n",
    "roc_auc_score : Area under curve (AUC) receiver operating cahracteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default classification reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the probabilities of default\n",
    "preds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n",
    "\n",
    "# Reassign loan status based on the threshold\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Print the row counts for each loan status\n",
    "print(preds_df['loan_status'].value_counts())\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df['loan_status'], target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Report metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df['loan_status'], target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the non-average values from the report\n",
    "print(precision_recall_fscore_support(y_test,preds_df['loan_status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually scoring credit models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions and store them in a variable\n",
    "preds = clf_logistic.predict_proba(X_test)\n",
    "\n",
    "# Print the accuracy score the model\n",
    "print(clf_logistic.score(X_test, y_test))\n",
    "\n",
    "# Plot the ROC curve of the probabilities of default\n",
    "prob_default = preds[:, 1]\n",
    "fallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\n",
    "plt.plot(fallout, sensitivity, color = 'darkorange')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Compute the AUC and store it in a variable\n",
    "auc = roc_auc_score(y_test, prob_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model discrimination and impact\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "| Predicted = 0| Predicted = 1 |\n",
    "| --- | --- |\n",
    "| True Negatives (Predicted = 0, Actual = 0)| False Positives (Predicted = 1, Actual - 0)|\n",
    "| False Negatives (Predicted = 0, Actual = 1)| True Positives (Predicted = 1, Actual = 1)|\n",
    "\n",
    "Precision_0 = TN / (TN + FN)\n",
    "\n",
    "Precision_1 = TP / (TP + FP)\n",
    "\n",
    "Recall_0 = TN / (TN + FP)\n",
    "\n",
    "Recall_1 = TP / (TP + FN) = Proportion of true defaults predicted = Sensitivity\n",
    "\n",
    "F1_score = 2 * (Precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholds and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for defaults to 0.5\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test,preds_df['loan_status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How thresholds affect performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the values of loan status based on the new threshold\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.4 else 0)\n",
    "\n",
    "# Store the number of loan defaults from the prediction data\n",
    "num_defaults = preds_df['loan_status'].value_counts()[1]\n",
    "\n",
    "# Store the default recall from the classification report\n",
    "default_recall = precision_recall_fscore_support(y_test,preds_df['loan_status'])[1][1]\n",
    "\n",
    "# Calculate the estimated impact of the new default recall rate\n",
    "print(avg_loan_amnt * num_defaults * (1 - default_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresh,def_recalls)\n",
    "plt.plot(thresh,nondef_recalls)\n",
    "plt.plot(thresh,accs)\n",
    "plt.xlabel(\"Probability Threshold\")\n",
    "plt.xticks(ticks)\n",
    "plt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees Using XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted trees with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees for defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "import xgboost as xgb\n",
    "clf_gbt = xgb.XGBClassifier().fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Predict with a model\n",
    "gbt_preds = clf_gbt.predict_proba(X_test)\n",
    "\n",
    "# Create dataframes of first five predictions, and first five true labels\n",
    "preds_df = pd.DataFrame(gbt_preds[:,1][0:5], columns = ['prob_default'])\n",
    "true_df = y_test.head()\n",
    "\n",
    "# Concatenate and print the two data frames for comparison\n",
    "print(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted portfolio performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of the portfolio data frame\n",
    "print(portfolio.head())\n",
    "\n",
    "# Create expected loss columns for each model using the formula\n",
    "portfolio['lr_expected_loss'] = portfolio['lr_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n",
    "portfolio['gbt_expected_loss'] = portfolio['gbt_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n",
    "\n",
    "# Print the sum of the expected loss for lr\n",
    "print('LR expected loss: ', np.sum(portfolio['lr_expected_loss']))\n",
    "\n",
    "# Print the sum of the expected loss for gbt\n",
    "print('GBT expected loss: ', np.sum(portfolio['gbt_expected_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing gradient boosted trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for loan status\n",
    "gbt_preds = clf_gbt.predict(X_test)\n",
    "\n",
    "# Check the values created by the predict method\n",
    "print(gbt_preds)\n",
    "\n",
    "# Print the classification report of the model\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column selection for credit risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column importance and default prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model on the training data\n",
    "clf_gbt = xgb.XGBClassifier().fit(X_train,np.ravel(y_train))\n",
    "\n",
    "# Print the column importances from the model\n",
    "print(clf_gbt.get_booster().get_score(importance_type = 'weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing column importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the X data with 2 columns\n",
    "clf_gbt2 = xgb.XGBClassifier().fit(X2_train,np.ravel(y_train))\n",
    "\n",
    "# Plot the column importance for this model\n",
    "xgb.plot_importance(clf_gbt2, importance_type = 'weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column selection and model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the loan_status using each model\n",
    "gbt_preds = gbt.predict(X_test)\n",
    "gbt2_preds = gbt2.predict(X2_test)\n",
    "\n",
    "# Print the classification report of the first model\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))\n",
    "\n",
    "# Print the classification report of the second model\n",
    "print(classification_report(y_test, gbt2_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation for credit models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating credit models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eval_metric': 'auc', 'objective': 'binary:logistic', 'seed': 123}\n",
    "\n",
    "# Set the values for number of folds and stopping iterations\n",
    "n_folds = 5\n",
    "early_stopping = 10\n",
    "\n",
    "# Create the DTrain matrix for XGBoost\n",
    "DTrain = xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "# Create the data frame of cross validations\n",
    "cv_df = xgb.cv(params, DTrain, num_boost_round = 5, nfold=n_folds,\n",
    "            early_stopping_rounds=early_stopping)\n",
    "\n",
    "# Print the cross validations data frame\n",
    "print(cv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits to cross-validation testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of the CV results data frame\n",
    "print(cv_results_big.head())\n",
    "\n",
    "# Calculate the mean of the test AUC scores\n",
    "print(np.mean(cv_results_big['test-auc-mean']).round(2))\n",
    "\n",
    "# Plot the test AUC scores for each iteration\n",
    "plt.plot(cv_results_big['test-auc-mean'])\n",
    "plt.title('Test AUC Score Over 600 Iterations')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Test AUC Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test AUC score never quite reaches 1.0 and begins to decrease slightly after 100 iterations. This is because this much cross-validation can actually cause the model to become overfit. So, there is a limit to how much cross-validation you should to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradient boosted tree model using two hyperparameters\n",
    "gbt = xgb.XGBClassifier(learning_rate = 0.1, max_depth = 7)\n",
    "\n",
    "# Calculate the cross validation scores for 4 folds\n",
    "cv_scores = cross_val_score(gbt, X_train, np.ravel(y_train), cv = 4)\n",
    "\n",
    "# Print the cross validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "# Print the average accuracy and standard deviation of the scores\n",
    "print(\"Average accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(),\n",
    "                                              cv_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance in loan data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sets for defaults and non-defaults\n",
    "nondefaults = X_y_train[X_y_train['loan_status'] == 0]\n",
    "defaults = X_y_train[X_y_train['loan_status'] == 1]\n",
    "\n",
    "# Undersample the non-defaults\n",
    "nondefaults_under = nondefaults.sample(count_default)\n",
    "\n",
    "# Concatenate the undersampled nondefaults with defaults\n",
    "X_y_train_under = pd.concat([nondefaults_under.reset_index(drop = True),\n",
    "                             defaults.reset_index(drop = True)], axis = 0)\n",
    "\n",
    "# Print the value counts for loan status\n",
    "print(X_y_train_under['loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampled tree performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the classification reports\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))\n",
    "print(classification_report(y_test, gbt2_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix for both old and new models\n",
    "print(confusion_matrix(y_test,gbt_preds))\n",
    "print(confusion_matrix(y_test,gbt2_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and compare the AUC scores of the old and new models\n",
    "print(roc_auc_score(y_test, gbt_preds))\n",
    "print(roc_auc_score(y_test, gbt2_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing model reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the logistic regression classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df_lr['loan_status'], target_names=target_names))\n",
    "\n",
    "# Print the gradient boosted tree classification report\n",
    "print(classification_report(y_test, preds_df_gbt['loan_status'], target_names=target_names))\n",
    "\n",
    "# Print the default F-1 scores for the logistic regression\n",
    "print(precision_recall_fscore_support(y_test,preds_df_lr['loan_status'], average = 'macro')[2])\n",
    "\n",
    "# Print the default F-1 scores for the gradient boosted tree\n",
    "print(precision_recall_fscore_support(y_test,preds_df_gbt['loan_status'], average = 'macro')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with ROCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC chart components\n",
    "fallout_lr, sensitivity_lr, thresholds_lr = roc_curve(y_test, clf_logistic_preds)\n",
    "fallout_gbt, sensitivity_gbt, thresholds_gbt = roc_curve(y_test, clf_gbt_preds)\n",
    "\n",
    "# ROC Chart with both\n",
    "plt.plot(fallout_lr, sensitivity_lr, color = 'blue', label='%s' % 'Logistic Regression')\n",
    "plt.plot(fallout_gbt, sensitivity_gbt, color = 'green', label='%s' % 'GBT')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='%s' % 'Random Prediction')\n",
    "plt.title(\"ROC Chart for LR and GBT on the Probability of Default\")\n",
    "plt.xlabel('Fall-out')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the logistic regression AUC with formatting\n",
    "print(\"Logistic Regression AUC Score: %0.2f\" % roc_auc_score(y_test, clf_logistic_preds))\n",
    "\n",
    "# Print the gradient boosted tree AUC with formatting\n",
    "print(\"Gradient Boosted Tree AUC Score: %0.2f\" % roc_auc_score(y_test, clf_gbt_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the calibration curve for the gradient boosted tree\n",
    "plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')    \n",
    "plt.plot(mean_pred_val_lr, frac_of_pos_lr,\n",
    "         's-', label='%s' % 'Logistic Regression')\n",
    "plt.plot(mean_pred_val_gbt, frac_of_pos_gbt,\n",
    "         's-', label='%s' % 'Gradient Boosted tree')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.xlabel('Average Predicted Probability')\n",
    "plt.legend()\n",
    "plt.title('Calibration Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit acceptance rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the statistics of the probabilities of default\n",
    "print(test_pred_df['prob_default'].describe())\n",
    "\n",
    "# Calculate the threshold for a 85% acceptance rate\n",
    "threshold_85 = np.quantile(test_pred_df['prob_default'], 0.85)\n",
    "\n",
    "# Apply acceptance rate threshold\n",
    "test_pred_df['pred_loan_status'] = test_pred_df['prob_default'].apply(lambda x: 1 if x > threshold_85 else 0)\n",
    "\n",
    "# Print the counts of loan status after the threshold\n",
    "print(test_pred_df['pred_loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing quantiles of acceptance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted probabilities of default\n",
    "plt.hist(clf_gbt_preds, color = 'blue', bins = 40)\n",
    "\n",
    "# Calculate the threshold with quantile\n",
    "threshold = np.quantile(clf_gbt_preds, 0.85)\n",
    "\n",
    "# Add a reference line to the plot for the threshold\n",
    "plt.axvline(x = threshold, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 rows of the new data frame\n",
    "print(test_pred_df.head())\n",
    "\n",
    "# Create a subset of only accepted loans\n",
    "accepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n",
    "\n",
    "# Calculate the bad rate\n",
    "print(np.sum(accepted_loans['true_loan_status']) / accepted_loans['true_loan_status'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance rate impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistics of the loan amount column\n",
    "print(test_pred_df['loan_amnt'].describe())\n",
    "\n",
    "# Store the average loan amount\n",
    "avg_loan = np.mean(test_pred_df['loan_amnt'])\n",
    "\n",
    "# Set the formatting for currency, and print the cross tab\n",
    "pd.options.display.float_format = '${:,.2f}'.format\n",
    "print(pd.crosstab(test_pred_df['true_loan_status'],\n",
    "                 test_pred_df['pred_loan_status_15']).apply(lambda x: x * avg_loan, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit strategy and minimum expected loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the strategy table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the arrays for the strategy table with a for loop\n",
    "for rate in accept_rates:\n",
    "    # Calculate the threshold for the acceptance rate\n",
    "    thresh = np.quantile(preds_df_gbt['prob_default'], rate).round(3)\n",
    "\n",
    "    # Add the threshold value to the list of thresholds\n",
    "    thresholds.append(np.quantile(preds_df_gbt['prob_default'], rate).round(3))\n",
    "\n",
    "    # Reassign the loan_status value using the threshold\n",
    "    test_pred_df['pred_loan_status'] = test_pred_df['prob_default'].apply(lambda x: 1 if x > thresh else 0)\n",
    "    \n",
    "    # Create a set of accepted loans using this acceptance rate\n",
    "    accepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n",
    "    \n",
    "    # Calculate and append the bad rate using the acceptance rate\n",
    "    bad_rates.append(np.sum((accepted_loans['true_loan_status']) / len(accepted_loans['true_loan_status'])).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame of the strategy table\n",
    "strat_df = pd.DataFrame(zip(accept_rates, thresholds, bad_rates),\n",
    "                        columns = ['Acceptance Rate','Threshold','Bad Rate'])\n",
    "\n",
    "# Print the entire table\n",
    "print(strat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions in the strategy table with a boxplot\n",
    "strat_df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the strategy curve\n",
    "plt.plot(strat_df['Acceptance Rate'], strat_df['Bad Rate'])\n",
    "plt.xlabel('Acceptance Rate')\n",
    "plt.ylabel('Bad Rate')\n",
    "plt.title('Acceptance and Bad Rates')\n",
    "plt.axes().yaxis.grid()\n",
    "plt.axes().xaxis.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated value profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line plot of estimated value\n",
    "plt.plot(strat_df['Acceptance Rate'],strat_df['Estimated Value'])\n",
    "plt.title('Estimated Value by Acceptance Rate')\n",
    "plt.xlabel('Acceptance Rate')\n",
    "plt.ylabel('Estimated Value')\n",
    "plt.axes().yaxis.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the row with the max estimated value\n",
    "print(strat_df.loc[strat_df['Estimated Value'] == np.max(strat_df['Estimated Value'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total expected loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows of the data frame\n",
    "print(test_pred_df.head())\n",
    "\n",
    "# Calculate the bank's expected loss and assign it to a new column\n",
    "test_pred_df['expected_loss'] = test_pred_df['prob_default'] * test_pred_df['loan_amnt'] * test_pred_df['loss_given_default']\n",
    "\n",
    "# Calculate the total expected loss to two decimal places\n",
    "tot_exp_loss = round(np.sum(test_pred_df['expected_loss']),2)\n",
    "\n",
    "# Print the total expected loss\n",
    "print('Total expected loss: ', '${:,.2f}'.format(tot_exp_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
