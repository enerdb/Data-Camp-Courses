{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk and return recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolio returns during the crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select portfolio asset prices for the middle of the crisis, 2008-2009\n",
    "asset_prices = portfolio.loc['2008-01-01':'2009-12-31']\n",
    "\n",
    "# Plot portfolio's asset prices during this time\n",
    "asset_prices.plot().set_ylabel(\"Closing Prices, USD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the portfolio's daily returns\n",
    "asset_returns = asset_prices.pct_change()\n",
    "portfolio_returns = asset_returns.dot(weights)\n",
    "\n",
    "# Plot portfolio returns\n",
    "portfolio_returns.plot().set_ylabel(\"Daily Return, %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asset covariance and portfolio volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the covariance matrix from portfolio asset's returns\n",
    "covariance = asset_returns.cov()\n",
    "\n",
    "# Annualize the covariance using 252 trading days per year\n",
    "covariance = covariance * 252\n",
    "\n",
    "# Display the covariance matrix\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display portfolio volatility for 2008 - 2009\n",
    "portfolio_variance = np.transpose(weights) @ covariance @ weights\n",
    "portfolio_volatility = np.sqrt(portfolio_variance)\n",
    "print(portfolio_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 30-day rolling window of portfolio returns\n",
    "returns_windowed = portfolio_returns.rolling(30)\n",
    "\n",
    "# Compute the annualized volatility series\n",
    "volatility_series = returns_windowed.std()*np.sqrt(252)\n",
    "\n",
    "# Plot the portfolio volatility\n",
    "volatility_series.plot().set_ylabel(\"Annualized Volatility, 30-day Window\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk factors and the financial crisis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency resampling primer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert daily returns to quarterly average returns\n",
    "returns_q = returns.resample('Q').mean()\n",
    "\n",
    "# Examine the beginning of the quarterly series\n",
    "print(returns_q.head())\n",
    "\n",
    "# Now convert daily returns to weekly minimum returns\n",
    "returns_w = returns.resample('W').min()\n",
    "\n",
    "# Examine the beginning of the weekly series\n",
    "print(returns_w.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing risk factor correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the daily portfolio_returns into quarterly average returns\n",
    "portfolio_q_average = portfolio_returns.resample('Q').mean().dropna()\n",
    "\n",
    "# Create a scatterplot between delinquency and quarterly average returns\n",
    "plot_average.scatter(mort_del, portfolio_q_average)\n",
    "\n",
    "# Transform daily portfolio_returns returns into quarterly minimum returns\n",
    "portfolio_q_min = portfolio_returns.resample('Q').min().dropna()\n",
    "\n",
    "# Create a scatterplot between delinquency and quarterly minimum returns\n",
    "plot_min.scatter(mort_del, portfolio_q_min)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares factor model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(port_q_mean, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(port_q_min, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Create the regression factor model and fit it to the data\n",
    "results = sm.OLS(vol_q_mean, mort_del).fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Portfolio Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with PyPortfolioOpt: returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the investment portfolio price data into the price variable.\n",
    "prices = pd.read_csv(\"portfolio.csv\")\n",
    "\n",
    "# Convert the 'Date' column to a datetime index\n",
    "prices['Date'] = pd.to_datetime(prices['Date'], format='%d/%m/%Y')\n",
    "prices.set_index(['Date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mean_historical_return method\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "\n",
    "# Compute the annualized average historical return\n",
    "mean_returns = mean_historical_return(prices, frequency = 252)\n",
    "\n",
    "# Plot the annualized average historical return\n",
    "plt.plot(mean_returns, linestyle = 'None', marker = 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with PyPortfolioOpt: covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CovarianceShrinkage object\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "\n",
    "# Create the CovarianceShrinkage instance variable\n",
    "cs = CovarianceShrinkage(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sample covariance matrix of returns\n",
    "sample_cov = prices.pct_change().cov() * 252\n",
    "\n",
    "# Compute the efficient covariance matrix of returns\n",
    "e_cov = cs.ledoit_wolf()\n",
    "\n",
    "# Display both the sample covariance_matrix and the efficient e_cov estimate\n",
    "print(\"Sample Covariance Matrix\\n\", sample_cov, \"\\n\")\n",
    "print(\"Efficient Covariance Matrix\\n\", e_cov, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the financial crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of time periods (or 'epochs')\n",
    "epochs = { 'before' : {'start': '1-1-2005', 'end': '31-12-2006'},\n",
    "           'during' : {'start': '1-1-2007', 'end': '31-12-2008'},\n",
    "           'after'  : {'start': '1-1-2009', 'end': '31-12-2010'}\n",
    "         }\n",
    "\n",
    "# Compute the efficient covariance for each epoch\n",
    "e_cov = {}\n",
    "for x in epochs.keys():\n",
    "  sub_price = prices.loc[epochs[x]['start']:epochs[x]['end']]\n",
    "  e_cov[x] = CovarianceShrinkage(sub_price).ledoit_wolf()\n",
    "\n",
    "# Display the efficient covariance matrices for all epochs\n",
    "print(\"Efficient Covariance Matrices\\n\", e_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The efficient frontier and the financial crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Crtical Line Algorithm object\n",
    "efficient_portfolio_during = CLA(returns_during, ecov_during)\n",
    "\n",
    "# Find the minimum volatility portfolio weights and display them\n",
    "print(efficient_portfolio_during.min_volatility())\n",
    "\n",
    "# Compute the efficient frontier\n",
    "(ret, vol, weights) = efficient_portfolio_during.efficient_frontier()\n",
    "\n",
    "# Add the frontier to the plot showing the 'before' and 'after' frontiers\n",
    "plt.scatter(vol, ret, s = 4, c = 'g', marker = '.', label = 'During')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal-oriented risk management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VaR for the Normal distribution\n",
    "Value at Risk (VaR) will be applied to a normal distribution\n",
    "\n",
    "In this exercise we assume loss is normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VaR measure at the 95% confidence level using norm.ppf() - percent point function\n",
    "VaR_95 = norm.ppf(0.95)\n",
    "\n",
    "# Create the VaR meaasure at the 5% significance level using numpy.quantile()\n",
    "draws = norm.rvs(size = 100000)\n",
    "VaR_99 = np.quantile(draws, 0.99)\n",
    "\n",
    "# Compare the 95% and 99% VaR\n",
    "print(\"95% VaR: \", VaR_95, \"; 99% VaR: \", VaR_99)\n",
    "\n",
    "# Plot the normal distribution histogram and 95% VaR measure\n",
    "plt.hist(draws, bins = 100)\n",
    "plt.axvline(x = VaR_95, c='r', label = \"VaR at 95% Confidence Level\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing CVaR and VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional value at risk (CVaR), or expected shortfall (ES), asks what the average loss will be, conditional upon losses exceeding some threshold at a certain confidence level. It uses VaR as a point of departure, but contains more information because it takes into consideration the tail of the loss distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance of the portfolio returns\n",
    "pm = portfolio_losses.mean()\n",
    "ps = portfolio_losses.std()\n",
    "\n",
    "# Compute the 95% VaR using the .ppf() - percent point function\n",
    "VaR_95 = norm.ppf(0.95, loc = pm, scale = ps)\n",
    "# Compute the expected tail loss and the CVaR in the worst 5% of cases\n",
    "tail_loss = norm.expect(lambda x: x, loc = pm, scale = ps, lb = VaR_95)\n",
    "CVaR_95 = (1 / (1 - 0.95)) * VaR_95\n",
    "\n",
    "# Plot the normal distribution histogram and add lines for the VaR and CVaR\n",
    "plt.hist(norm.rvs(size = 100000, loc = pm, scale = ps), bins = 100)\n",
    "plt.axvline(x = VaR_95, c='r', label = \"VaR, 95% confidence level\")\n",
    "plt.axvline(x = CVaR_95, c='g', label = \"CVaR, worst 5% of outcomes\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although VaR and CVaR are similar (and only one letter apart!), it's generally the case that CVaR is the preferred risk measure for risk management. One reason is that it is affected by the tail of the loss distribution, while VaR is a static value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk exposure and loss\n",
    "\n",
    "Risk exposure = probability of loss x loss measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VaR and risk exposure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Student's t-distribution\n",
    "from scipy.stats import t\n",
    "\n",
    "# Create rolling window parameter list\n",
    "mu = losses.rolling(30).mean()\n",
    "sigma = losses.rolling(30).std()\n",
    "rolling_parameters = [(29, mu[i], s) for i,s in enumerate(sigma)]\n",
    "\n",
    "# Compute the 99% VaR array using the rolling window parameters\n",
    "VaR_99 = np.array( [ t.ppf(0.99, *params) \n",
    "                    for params in rolling_parameters ] )\n",
    "\n",
    "# Plot the minimum risk exposure over the 2005-2010 time period\n",
    "plt.plot(losses.index, 0.01 * VaR_99 * 100000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVaR and risk exposure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Student's t distribution to crisis losses\n",
    "p = t.fit(crisis_losses)\n",
    "\n",
    "# Compute the VaR_99 for the fitted distribution\n",
    "VaR_99 = t.ppf(0.99, *p)\n",
    "\n",
    "# Use the fitted parameters and VaR_99 to compute CVaR_99\n",
    "tail_loss = t.expect(lambda y: y, args = (p[0],), loc = p[1], scale = p[2], lb = VaR_99 )\n",
    "CVaR_99 = (1 / (1 - 0.99)) * tail_loss\n",
    "print(CVaR_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk management using VaR & CVaR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VaR from a fitted distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fitted distribution with a plot\n",
    "x = np.linspace(-0.25,0.25,1000)\n",
    "plt.plot(x,fitted.evaluate(x))\n",
    "plt.show()\n",
    "\n",
    "# Create a random sample of 100,000 observations from the fitted distribution\n",
    "sample = fitted.resample(100000)\n",
    "\n",
    "# Compute and display the 95% VaR from the random sample\n",
    "VaR_95 = np.quantile(sample, 0.95)\n",
    "print(VaR_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing CVaR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EfficientFrontier class\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "# Import the negative_cvar objective function\n",
    "from pypfopt.objective_functions import negative_cvar\n",
    "\n",
    "# Create the efficient frontier instance\n",
    "ef = EfficientFrontier(None, e_cov)\n",
    "\n",
    "# Find the cVar-minimizing portfolio weights at the default 95% confidence level\n",
    "optimal_weights = ef.custom_objective(negative_cvar, returns)\n",
    "\n",
    "# Display the optimal weights\n",
    "print(optimal_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVaR risk management and the crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the efficient portfolio dictionary\n",
    "ef_dict = {}\n",
    "\n",
    "# For each epoch, assign an efficient frontier instance to ef\n",
    "for x in ['before', 'during', 'after']: \n",
    "    ef_dict[x] = EfficientFrontier(None, e_cov_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary of optimal weights\n",
    "optimal_weights_dict = {}\n",
    "\n",
    "# Find and display the CVaR-minimizing portfolio weights at the default 95% confidence level\n",
    "for x in ['before', 'during', 'after']:\n",
    "    optimal_weights_dict[x] = ef_dict[x].custom_objective(negative_cvar, returns_dict[x])\n",
    "\n",
    "# Compare the CVaR-minimizing weights to the minimum volatility weights for the 'before' epoch\n",
    "print(\"CVaR:\\n\", pd.DataFrame.from_dict(optimal_weights_dict['before']), \"\\n\")\n",
    "print(\"Min Vol:\\n\", pd.DataFrame.from_dict(min_vol_dict['before']), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio hedging: offsetting risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black-Scholes options pricing\n",
    "https://assets.datacamp.com/production/repositories/5157/datasets/f275319f10cf6bd59d01a98eb8d960178eedc945/black_scholes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Arguments:\n",
    "    S           -- the current spot price of the underlying stock\n",
    "    X           -- the option strike price\n",
    "    T           -- the time until maturity (in fractions of a year)\n",
    "    r           -- the risk-free interest rate \n",
    "    sigma       -- the returns volatility of the underlying stock\n",
    "    option_type -- the option type, either 'call' or 'put'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the volatility as the annualized standard deviation of IBM returns\n",
    "sigma = np.sqrt(252) * IBM_returns.std()\n",
    "\n",
    "# Compute the Black-Scholes option price for this volatility\n",
    "value_s = black_scholes(S = 90, X = 80, T = 0.5, r = 0.02, \n",
    "                        sigma = sigma, option_type = \"call\")\n",
    "\n",
    "# Compute the Black-Scholes option price for twice the volatility\n",
    "value_2s = black_scholes(S = 90, X = 80, T = 0.5, r = 0.02, \n",
    "                sigma = 2*sigma, option_type = \"call\")\n",
    "\n",
    "# Display and compare both values\n",
    "print(\"Option value for sigma: \", value_s, \"\\n\",\n",
    "      \"Option value for 2 * sigma: \", value_2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options pricing and the underlying asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 100 observations of IBM data\n",
    "IBM_spot = IBM[:100]\n",
    "\n",
    "# Initialize the European put option values array\n",
    "option_values = np.zeros(IBM_spot.size)\n",
    "\n",
    "# Iterate through IBM's spot price and compute the option values\n",
    "for i,S in enumerate(IBM_spot.values):\n",
    "    option_values[i] = black_scholes(S = S, X = 140, T = 0.5, r = 0.02, \n",
    "                        sigma = sigma, option_type = \"put\")\n",
    "\n",
    "# Display the option values array\n",
    "option_axis.plot(option_values, color = \"red\", label = \"Put Option\")\n",
    "option_axis.legend(loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using options for hedging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the annualized standard deviation of `IBM` returns\n",
    "sigma = np.sqrt(252) * IBM_returns.std()\n",
    "\n",
    "# Compute the Black-Scholes value at IBM spot price 70\n",
    "value = black_scholes(S = 70, X = 80, T = 0.5, r = 0.02, \n",
    "                      sigma = sigma, option_type = \"put\")\n",
    "# Find the delta of the option at IBM spot price 70\n",
    "delta = bs_delta(S = 70, X = 80, T = 0.5, r = 0.02, \n",
    "                 sigma = sigma, option_type = \"put\")\n",
    "\n",
    "# Find the option value change when the price of IBM falls to 69.5\n",
    "value_change = black_scholes(S = 69.5, X = 80, T = 0.5, r = 0.02, \n",
    "                             sigma = sigma, option_type = \"put\") - value\n",
    "\n",
    "# Show that the sum is close to zero\n",
    "print( (69.5 - 70) + (1/delta) * value_change )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating and identifying risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation: Normal\n",
    "\n",
    "Test with the scipy.stats.anderson Anderson-Darling test. If the test result is statistically different from zero, this indicates the data is not Normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Normal distribution and skewness test from scipy.stats\n",
    "from scipy.stats import norm, anderson\n",
    "\n",
    "# Fit portfolio losses to the Normal distribution\n",
    "params = norm.fit(losses)\n",
    "\n",
    "# Compute the 95% VaR from the fitted distribution, using parameter estimates\n",
    "VaR_95 = norm.ppf(0.95, *params)\n",
    "print(\"VaR_95, Normal distribution: \", VaR_95)\n",
    "\n",
    "# Test the data for Normality\n",
    "print(\"Anderson-Darling test result: \", anderson(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation: Skewed Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the skew-normal distribution and skewness test from scipy.stats\n",
    "from scipy.stats import skewnorm, skewtest\n",
    "\n",
    "# Test the data for skewness\n",
    "print(\"Skewtest result: \", skewtest(losses))\n",
    "\n",
    "# Fit the portfolio loss data to the skew-normal distribution\n",
    "params = skewnorm.fit(losses)\n",
    "\n",
    "# Compute the 95% VaR from the fitted distribution, using parameter estimates\n",
    "VaR_95 = skewnorm.ppf(0.95, *params)\n",
    "print(\"VaR_95 from skew-normal: \", VaR_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical and Monte Carlo Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Simulation\n",
    "Historical simulation of VaR assumes that the distribution of historical losses is the same as the distribution of future losses. We'll test if this is true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create portfolio returns for the two sub-periods using the list of asset returns\n",
    "portfolio_returns = np.array([ x.dot(weights) for x in asset_returns])\n",
    "\n",
    "# Derive portfolio losses from portfolio returns\n",
    "losses = - portfolio_returns\n",
    "\n",
    "# Find the historical simulated VaR estimates\n",
    "VaR_95 = [np.quantile(x, 0.95) for x in losses]\n",
    "\n",
    "# Display the VaR estimates\n",
    "print(\"VaR_95, 2005-2006: \", VaR_95[0], '; VaR_95, 2007-2009: ', VaR_95[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize daily cumulative loss for the assets, across N runs\n",
    "daily_loss = np.zeros((4,N))\n",
    "\n",
    "# Create the Monte Carlo simulations for N runs\n",
    "for n in range(N):\n",
    "    # Compute simulated path of length total_steps for correlated returns\n",
    "    correlated_randomness = e_cov @ norm.rvs(size = (4,total_steps))\n",
    "    # Adjust simulated path by total_steps and mean of portfolio losses\n",
    "    steps = 1/total_steps\n",
    "    minute_losses = mu * steps + correlated_randomness * np.sqrt(steps)\n",
    "    daily_loss[:, n] = minute_losses.sum(axis=1)\n",
    "    \n",
    "# Generate the 95% VaR estimate\n",
    "losses = weights @ daily_loss\n",
    "print(\"Monte Carlo VaR_95 estimate: \", np.quantile(losses, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural breaks\n",
    "\n",
    "Occurs when the distribution parameters changes due to an event over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crisis structural break: I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of quarterly minimum portfolio returns\n",
    "plt.plot(port_q_min, label=\"Quarterly minimum return\")\n",
    "\n",
    "# Create a plot of quarterly mean volatility\n",
    "plt.plot(vol_q_mean, label=\"Quarterly mean volatility\")\n",
    "\n",
    "# Create legend and plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crisis structural break: II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels API to be able to run regressions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the regression\n",
    "mort_del = sm.add_constant(mort_del)\n",
    "\n",
    "# Regress quarterly minimum portfolio returns against mortgage delinquencies\n",
    "result = sm.OLS(port_q_min, mort_del).fit()\n",
    "\n",
    "# Retrieve the sum-of-squared residuals\n",
    "ssr_total = result.ssr\n",
    "print(\"Sum-of-squared residuals, 2005-2010: \", ssr_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crisis structural break: III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept constants to each sub-period 'before' and 'after'\n",
    "before_with_intercept = sm.add_constant(before['mort_del'])\n",
    "after_with_intercept  = sm.add_constant(after['mort_del'])\n",
    "\n",
    "# Fit OLS regressions to each sub-period\n",
    "r_b = sm.OLS(before['returns'], before_with_intercept).fit()\n",
    "r_a = sm.OLS(after['returns'],  after_with_intercept).fit()\n",
    "\n",
    "# Get sum-of-squared residuals for both regressions\n",
    "ssr_before = r_b.ssr\n",
    "ssr_after = r_a.ssr\n",
    "# Compute and display the Chow test statistic\n",
    "numerator = ((ssr_total - (ssr_before + ssr_after)) / 2)\n",
    "denominator = ((ssr_before + ssr_after) / (24 - 4))\n",
    "print(\"Chow test statistic: \", numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility and extreme values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility and structural breaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the time series of returns with and without Citibank\n",
    "ret_with_citi = prices_with_citi.pct_change().dot(weights_with_citi)\n",
    "ret_without_citi = prices_without_citi.pct_change().dot(weights_without_citi)\n",
    "\n",
    "# Find the average 30-day rolling window volatility as the standard deviation\n",
    "vol_with_citi = ret_with_citi.rolling(30).std().dropna().rename(\"With Citi\")\n",
    "vol_without_citi = ret_without_citi.rolling(30).std().dropna().rename(\"Without Citi\")\n",
    "\n",
    "# Combine two volatilities into one Pandas DataFrame\n",
    "vol = pd.concat([vol_with_citi, vol_without_citi], axis=1)\n",
    "\n",
    "# Plot volatilities over time\n",
    "vol.plot().set_ylabel(\"Losses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme values and backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 95% VaR on 2009-2010 losses\n",
    "VaR_95 = np.quantile(estimate_data, 0.95)\n",
    "\n",
    "# Find backtest_data exceeding the 95% VaR\n",
    "extreme_values = backtest_data[backtest_data > VaR_95]\n",
    "\n",
    "# Compare the fraction of extreme values for 2007-2008 to the Var_95 estimate\n",
    "print(\"VaR_95: \", VaR_95, \"; Backtest: \", len(extreme_values) / len(backtest_data) )\n",
    "\n",
    "# Plot the extreme values and look for clustering\n",
    "plt.stem(extreme_values.index, extreme_values)\n",
    "plt.ylabel(\"Extreme values > VaR_95\"); plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced risk management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme value theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block maxima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data into weekly blocks\n",
    "weekly_maxima = losses.resample(\"W\").max()\n",
    "\n",
    "# Plot the resulting weekly maxima\n",
    "axis_1.plot(weekly_maxima, label = \"Weekly Maxima\")\n",
    "axis_1.legend()\n",
    "plt.figure(\"weekly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data into monthly blocks\n",
    "monthly_maxima = losses.resample(\"M\").max()\n",
    "\n",
    "# Plot the resulting monthly maxima\n",
    "axis_2.plot(monthly_maxima, label = \"Monthly Maxima\")\n",
    "axis_2.legend()\n",
    "plt.figure(\"monthly\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme events during the crisis\n",
    "\n",
    "You can use the Generalized Extreme Value (GEV) distribution to examine extreme values in the losses of General Electric (GE) during the financial crisis in 2008 and 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log daily losses of GE over the period 2007-2009\n",
    "losses.plot()\n",
    "\n",
    "# Find all daily losses greater than 10%\n",
    "extreme_losses = losses[losses > 0.1]\n",
    "\n",
    "# Scatter plot the extreme losses\n",
    "extreme_losses.plot(style='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit extreme distribution to weekly maximum of losses\n",
    "fitted = genextreme.fit(weekly_max)\n",
    "\n",
    "# Plot extreme distribution with weekly max losses historgram\n",
    "x = np.linspace(min(weekly_max), max(weekly_max), 100)\n",
    "plt.plot(x, genextreme.pdf(x, *fitted))\n",
    "plt.hist(weekly_max, 50, density = True, alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEV risk estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weekly block maxima for GE's stock\n",
    "weekly_maxima = losses.resample(\"W\").max()\n",
    "\n",
    "# Fit the GEV distribution to the maxima\n",
    "p = genextreme.fit(weekly_maxima)\n",
    "\n",
    "# Compute the 99% VaR (needed for the CVaR computation)\n",
    "VaR_99 = genextreme.ppf(0.99, *p)\n",
    "\n",
    "# Compute the 99% CVaR estimate\n",
    "CVaR_99 = (1 / (1 - 0.99)) * genextreme.expect(lambda x: x, \n",
    "           args=(p[0],), loc = p[1], scale = p[2], lb = VaR_99)\n",
    "\n",
    "# Display the covering loss amount\n",
    "print(\"Reserve amount: \", 1000000 * CVaR_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation\n",
    "\n",
    "Risk factors que be assumed (Normal, T, etc), Fitted (parametric or Monte Carlo), or ignored (historical simulation)\n",
    "\n",
    "Non parametric estimation smoths (filters) the data to better fit a distribution.\n",
    "\n",
    "A histogram can be cuted in pieces and a distribution may be fitted to each slice (kernel). The gaussian kernel is the most famous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE of a loss distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fitted T distribution over losses\n",
    "params = t.fit(losses)\n",
    "\n",
    "# Generate a Gaussian kernal density estimate over losses\n",
    "kde = gaussian_kde(losses)\n",
    "\n",
    "# Add the PDFs of both estimates to a histogram, and display\n",
    "loss_range = np.linspace(np.min(losses), np.max(losses), 1000)\n",
    "axis.plot(loss_range, t.pdf(loss_range, *params), label = 'T distribution')\n",
    "axis.plot(loss_range, kde.pdf(loss_range), label = 'Gaussian KDE')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVaR and loss cover selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the VaR as a quantile of random samples from the distributions\n",
    "VaR_99_T   = np.quantile(t.rvs(size=1000, *p), 0.99)\n",
    "VaR_99_KDE = np.quantile(kde.resample(size=1000), 0.99)\n",
    "\n",
    "# Find the expected tail losses, with lower bounds given by the VaR measures\n",
    "integral_T   = t.expect(lambda x: x, args = (p[0],), loc = p[1], scale = p[2], lb = VaR_99_T)\n",
    "integral_KDE = kde.expect(lambda x: x, lb = VaR_99_KDE)\n",
    "\n",
    "# Create the 99% CVaR estimates\n",
    "CVaR_99_T   = (1 / (1 - 0.99)) * integral_T\n",
    "CVaR_99_KDE = (1 / (1 - 0.99)) * integral_KDE\n",
    "\n",
    "# Display the results\n",
    "print(\"99% CVaR for T: \", CVaR_99_T, \"; 99% CVaR for KDE: \", CVaR_99_KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network risk management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer neural networks\n",
    "\n",
    "To become comfortable using neural networks it will be helpful to start with a simple approximation of a function.\n",
    "\n",
    "You'll train a neural network to approximate a mapping between an input, x, and an output, y. They are related by the square root function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training values from the square root function\n",
    "y = np.sqrt(x)\n",
    "\n",
    "# Create the neural network with one hidden layer of 16 neurons, one input value, and one output value\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=1, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Train the network\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(x, y, epochs=100)\n",
    "\n",
    "## Plot the resulting approximation and the training values\n",
    "plt.plot(x, y, x, model.predict(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asset price prediction\n",
    "\n",
    "This prediction uses the price of other assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output data\n",
    "training_input = prices.drop('Morgan Stanley', axis=1)\n",
    "training_output = prices['Morgan Stanley']\n",
    "\n",
    "# Create and train the neural network with two hidden layers\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=3, activation='sigmoid'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='rmsprop')\n",
    "model.fit(training_input, training_output, epochs=100)\n",
    "\n",
    "# Scatter plot of the resulting model prediction\n",
    "axis.scatter(training_output, model.predict(training_input)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time risk management\n",
    "\n",
    "We used the pre-treined model because training would take so long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = 4, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'relu'))\n",
    "\n",
    "# Use the pre-trained model to predict portfolio weights given new asset returns\n",
    "asset_returns = np.array([0.001060, 0.003832, 0.000726, -0.002787])\n",
    "asset_returns.shape = (1,4)\n",
    "print(\"Predicted minimum volatility portfolio: \", pre_trained_model.predict(asset_returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
